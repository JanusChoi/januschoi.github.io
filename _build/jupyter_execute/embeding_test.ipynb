{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca5f307-c6f5-433e-a57c-182c2af14e17",
   "metadata": {},
   "source": [
    "# 让GPT帮你读文档：一种简单的实现方法\n",
    "\n",
    "GPT-4阅读文档的原理与人类阅读类似。想象一下，当您拿到一份数十页的PDF文件时，您会先关注哪些部分？摘要、总结以及目录结构。接着，您会在心里提出若干问题（大约3-5个），并带着这些问题继续阅读。\n",
    "\n",
    "为了借助GPT-4实现高效阅读，并尝试突破单次 token 数量限制，我们需要使用官方提供的 embedding 工具箱。简单来说，embedding 的原理就是将一段文本压缩成一组向量数据，就像是将文章片段存储到大脑中。\n",
    "\n",
    "因此，我们的程序分为以下几个步骤：\n",
    "\n",
    "第一步：清洗并切片PDF文档\n",
    "\n",
    "对PDF文档进行清洗，去除重复的页眉、页脚以及目录中的过长连字符，以尽量减少API调用次数（毕竟每次调用都需要花费）。\n",
    "将文档按段落切片，对于过长的段落则拆分成两部分。\n",
    "将所有切片输入API生成embedding，并将其存储到 parquet 文件格式中，便于后续复用。\n",
    "\n",
    "第二步：生成概述和提出问题\n",
    "\n",
    "读取文档前10页（不超过4096个token）的数据量，提交给GPT-4以生成概述。\n",
    "让GPT-4根据概述提出五个相关问题。至此，阅读文档和提出问题的第一步已完成。\n",
    "\n",
    "第三步：回答问题\n",
    "\n",
    "以“问题一”为例，我们需要执行以下操作：\n",
    "将“问题一”输入API生成embedding-1。\n",
    "将embedding-1与之前生成的embedding集合进行一一比对，计算余弦相似度。\n",
    "对数据进行排序，筛选出Top N条相似的embedding。\n",
    "将第3步筛选出的embedding原文提交给GPT-4，让其生成一段通顺的回答。\n",
    "输出第3步Top N的embedding原文，以便了解答案来源。\n",
    "\n",
    "重复以上过程四次，即可让GPT-4回答五个问题。将所有内容整合到一个Markdown文件中保存即可。\n",
    "\n",
    "第四步：提供额外的问题支持\n",
    "\n",
    "有时，我们对GPT-4提出的问题可能并不满意，因此需要继续向文档提问。在这里，我们使用Python的input函数在命令行中执行上述提问流程。当我们提出所有想要问的问题后，这些后续问题的回答将整合到另一个Markdown文件中，并保存在与PDF文件同一路径下。\n",
    "通过以上步骤，我们可以利用GPT-4更高效地阅读文档，并对文档内容进行深入理解。这种方法既节省了时间，又提高了工作效率，使得我们能够更轻松地处理大量文档资料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6410a-df96-4553-8503-1e76eb25a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import openai\n",
    "import os\n",
    "import logging as logger\n",
    "from flask_cors import CORS\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc4008-6bc3-423e-bb4f-e84625207400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(df, query, n=3, pprint=True):\n",
    "    query_embedding = get_embedding(\n",
    "        query,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))\n",
    "\n",
    "    results = df.sort_values(\"similarity\", ascending=False, ignore_index=True)\n",
    "    results = results.head(n)\n",
    "    global sources\n",
    "    sources = []\n",
    "    for i in range(n):\n",
    "        sources.append({'Page '+str(results.iloc[i]['page']): results.iloc[i]['text'][:150]+'...'})\n",
    "    return results.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca4af9-fa0a-42c0-a1bd-50db353868fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(df, user_input, strategy=None):\n",
    "    result = search_embeddings(df, user_input)\n",
    "    if strategy == \"paper\":\n",
    "        prompt = \"\"\"You are a large language model whose expertise is reading and summarizing scientific papers.\n",
    "        You are given a query and a series of text embeddings from a paper in order of their cosine similarity to the query.\n",
    "        You must take the given embeddings and return a very detailed summary of the paper that answers the query.\n",
    "            Given the question: \"\"\"+ user_input + \"\"\"\n",
    "\n",
    "            and the following embeddings as data: \n",
    "\n",
    "            1.\"\"\" + str(result.iloc[0]['text']) + \"\"\"\n",
    "            2.\"\"\" + str(result.iloc[1]['text']) + \"\"\"\n",
    "\n",
    "            Return a concise and accurate answer:\"\"\"\n",
    "    elif strategy == \"handbook\":\n",
    "        prompt = \"\"\"You are a large language model whose expertise is reading and summarizing financial handbook.\n",
    "        You are given a query and a series of text embeddings from a handbook in order of their cosine similarity to the query.\n",
    "        You must take the given embeddings and return a very detailed answer in Chinese of the handbook that answers the query.\n",
    "        If not necessary, your answer please use the original text as much as possible.\n",
    "        You should also ensure that your response is written in clear and concise Chinese, using appropriate grammar and vocabulary.  \n",
    "        Additionally, your response should focus on answering the specific query provided..\n",
    "            Given the question: \"\"\"+ user_input + \"\"\"\n",
    "            and the following embeddings as data: \n",
    "\n",
    "            1.\"\"\" + str(result.iloc[0]['text']) + \"\"\"\n",
    "            2.\"\"\" + str(result.iloc[1]['text']) + \"\"\"\n",
    "\n",
    "            Return a concise and accurate answer:\"\"\"\n",
    "    elif strategy == \"contract\":\n",
    "        prompt = \"\"\"As a large language model specializing in reading and summarizing, your task is to read a query and a sequence of text inputs sorted by their cosine similarity to the query.\n",
    "         Your goal is to provide a Chinese answer to the query using the given padding. If possible, please use the original text of your answer. \n",
    "         Please ensure that your response adheres to the terms of the agreement. Your response should focus on addressing the specific query provided, \n",
    "         providing relevant information and details based on the input texts' content. You should also strive for clarity and conciseness in your response, \n",
    "         summarizing key points while maintaining accuracy and relevance. Please note that you should prioritize understanding the context and meaning \n",
    "         behind both the query and input texts before generating a response.\n",
    "            Given the question: \"\"\"+ user_input + \"\"\"\n",
    "            and the following embeddings as data: \n",
    "\n",
    "            1.\"\"\" + str(result.iloc[0]['text']) + \"\"\"\n",
    "            2.\"\"\" + str(result.iloc[1]['text']) + \"\"\"\n",
    "\n",
    "            Return a concise and accurate answer:\"\"\"\n",
    "    else:\n",
    "        prompt = \"\"\"As a language model specialized in reading and summarizing documents,your task is to provide a concise answer in Chinese based on a given query and a series of text embeddings from the document.The embeddings are provided in order of their cosine similarity to the query. Your response should use as much original text as possible.Your answer should be highly concise and accurate, providing relevant information that directly answers the query.You should ensure that your response is written in clear and concise Chinese, using appropriate grammar and vocabulary.Please note that you must use the provided text embeddings to generate your response, which means you will need to understand how they relate to the original document.Your response should focus on answering the specific query provided.Given the question: \"\"\"+ user_input + \"\"\" and the following embeddings as data: 1.\"\"\" + str(result.iloc[0]['text']) + \"\"\"2.\"\"\" + str(result.iloc[1]['text'])\n",
    "    logger.info('Done creating prompt')\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dfb25-6625-4e03-abc5-fade6051e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行环境初始化\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1088\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1088\"\n",
    "\n",
    "openai.organization = \"org-your_org\"\n",
    "openai.api_key = \"sk-your_api_key\"\n",
    "\n",
    "full_report = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72933db1-2ac1-4813-880c-e0c3039da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf文档输入\n",
    "\n",
    "pdf_path = \"/Users/januswing/data/PDFReadTest/大数据在路况监测中的应用（英） PIARC 2023.pdf\"\n",
    "file_name_prefix = pdf_path[:-4]\n",
    "pdf = pdfplumber.open(pdf_path)\n",
    "number_of_pages = len(pdf.pages)\n",
    "full_report = \"{}分析文档{}，总页数{}\\n\\n\".format(full_report, pdf_path, number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578967-0ded-4b3c-ad96-6cd8c43be189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取pdf内容\n",
    "import re\n",
    "\n",
    "paper_text = []\n",
    "pre_read_content = \"\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    \n",
    "    # 对每一页的文本进行处理\n",
    "    for i in range(len(pdf.pages)):\n",
    "        if i <= 10 and len(pre_read_content) < 3000:\n",
    "            pre_read_content = '{}{}'.format(pre_read_content, pdf.pages[i].extract_text()) # 读取前10页的内容\n",
    "        else:\n",
    "            pre_read_content = pre_read_content[0:3000]\n",
    "        page = pdf.pages[i]\n",
    "        words = page.extract_words(extra_attrs=['size'])\n",
    "        blob_font_size = None\n",
    "        blob_text = ''\n",
    "        processed_text = []\n",
    "\n",
    "        for word in words:\n",
    "            if word['size'] == blob_font_size:\n",
    "                blob_text += f\" {word['text']}\"\n",
    "                if len(blob_text) >= 2000: #这个数值控制的是一个段落可能最小的长度\n",
    "                    processed_text.append({\n",
    "                        'fontsize': blob_font_size,\n",
    "                        'text': re.sub(r'\\.{2,}', ' ', blob_text),\n",
    "                        'page': i\n",
    "                    })\n",
    "                    blob_font_size = None\n",
    "                    blob_text = ''\n",
    "            else:\n",
    "                if blob_font_size is not None and len(blob_text) >= 1:\n",
    "                    processed_text.append({\n",
    "                        'fontsize': blob_font_size,\n",
    "                        'text': re.sub(r'\\.{2,}', ' ', blob_text),\n",
    "                        'page': i\n",
    "                    })\n",
    "                blob_font_size = word['size']\n",
    "                blob_text = word['text']\n",
    "            paper_text += processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad4d86-f1f0-4860-9676-35d835083c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成embeddings\n",
    "embeddings_file_path = '{}.parquet'.format(file_name_prefix)\n",
    "if os.path.exists(embeddings_file_path):\n",
    "    df = pd.read_parquet(embeddings_file_path, engine='pyarrow')\n",
    "else:\n",
    "    filtered_pdf= []\n",
    "    for row in paper_text:\n",
    "        if len(row['text']) < 30:\n",
    "            continue\n",
    "        if len(row['text']) > 8000:\n",
    "            row['text'] = row['text'][:8000]\n",
    "        filtered_pdf.append(row)\n",
    "    df = pd.DataFrame(filtered_pdf)\n",
    "    df = df.drop_duplicates(subset=['text', 'page'], keep='first')\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "    embedding_model = \"text-embedding-ada-002\"\n",
    "    embeddings = []\n",
    "    for text in tqdm(df.text.values, desc=\"Generating embeddings\"):\n",
    "            embeddings.append(get_embedding(text, engine=embedding_model))\n",
    "    df[\"embeddings\"] = embeddings\n",
    "    # 保存留后续复用embeddings\n",
    "    df.to_parquet(embeddings_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38471ac2-857b-4bf6-b245-551bb92a7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出文章概述\n",
    "prompt_messages = []\n",
    "prefix = '你是信息分析员'\n",
    "i_say = f'对下面的文章片段用中文做概述，文章内容是 ```{pre_read_content}```'\n",
    "\n",
    "system_content = {\"role\": \"system\", \"content\": prefix}\n",
    "user_content_final = {\"role\": \"user\", \"content\": i_say}\n",
    "prompt_messages.append(system_content)\n",
    "prompt_messages.append(user_content_final)\n",
    "r = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt_messages)\n",
    "res = json.loads(str(r))\n",
    "overview = res['choices'][0]['message']['content']\n",
    "full_report = \"{}{}\\n\\n\".format(full_report, overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd6103-233f-4dad-88c1-2d81ef42d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出问题\n",
    "i_say = f'对这篇文章提出可能的五个问题'\n",
    "\n",
    "user_content_final = {\"role\": \"user\", \"content\": i_say}\n",
    "prompt_messages.append(user_content_final)\n",
    "r = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt_messages)\n",
    "res = json.loads(str(r))\n",
    "questions = res['choices'][0]['message']['content']\n",
    "full_report = \"{}可能的问题：{}\\n\\n\".format(full_report, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acfecf-eeb1-4382-b703-e0cea159701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对问题逐一回答\n",
    "qlist = questions.split('\\n')\n",
    "qnum = 0\n",
    "for question in qlist:\n",
    "    prompt_messages = []\n",
    "#     if qnum > 0:\n",
    "#         prompt_messages.pop()\n",
    "    prefix = \"\"\n",
    "    i_say = prefix + create_prompt(df, question)\n",
    "\n",
    "    user_content_final = {\"role\": \"user\", \"content\": i_say}\n",
    "    prompt_messages.append(user_content_final)\n",
    "    r = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt_messages)\n",
    "\n",
    "    answer = r.choices[0]['message']\n",
    "    res = json.loads(str(r))\n",
    "    res_status = res['choices'][0]['finish_reason']\n",
    "    res_content = res['choices'][0]['message']['content']\n",
    "    answer = res_content\n",
    "    response = {'answer': answer, 'sources': sources}\n",
    "    full_report = \"{}回答{}：\\n{}\\n\\n\".format(full_report, question, response['answer'])\n",
    "    for source in response['sources']:\n",
    "        for key, value in source.items():\n",
    "            full_report = \"{}来自原文{}:{}\\n\\n\".format(full_report, key, value)\n",
    "    qnum = qnum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aed4a-7eaa-4f1b-80e9-455b7564316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = \"{}_{}.md\".format(file_name_prefix, current_date_time)\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(full_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3730e5-bdbb-43ed-8ea2-302bebca6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom question answering\n",
    "current_date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "history_filename = \"{}_chat_{}.md\".format(file_name_prefix, current_date_time)\n",
    "\n",
    "def ask_question(df, history_filename):\n",
    "    question = input(\"Please enter your question: \")\n",
    "    \n",
    "    if question.lower() == \"exit\":\n",
    "        return None\n",
    "    \n",
    "    prompt_messages = []\n",
    "    history = \"\"\n",
    "\n",
    "    prefix = \"\"\n",
    "    i_say = prefix + create_prompt(df, question)\n",
    "\n",
    "    user_content_final = {\"role\": \"user\", \"content\": i_say}\n",
    "    prompt_messages.append(user_content_final)\n",
    "    r = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt_messages)\n",
    "\n",
    "    answer = r.choices[0]['message']\n",
    "    res = json.loads(str(r))\n",
    "    res_status = res['choices'][0]['finish_reason']\n",
    "    res_content = res['choices'][0]['message']['content']\n",
    "    answer = res_content\n",
    "    response = {'answer': answer, 'sources': sources}\n",
    "    print(\"{}的回答：\\n{}\\n\\n\".format(question, response['answer']))\n",
    "    history = \"{}{}\".format(history, \"{}的回答：\\n{}\\n\\n\".format(question, response['answer']))\n",
    "    for source in response['sources']:\n",
    "        for key, value in source.items():\n",
    "            print(\"来自原文{}:{}\\n\\n\".format(key, value))\n",
    "            history = \"{}{}\".format(history, \"来自原文{}:{}\\n\\n\".format(key, value))\n",
    "    # Save conversation history\n",
    "    with open(history_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336c472-193b-4a70-bc94-96a49012017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        ask_question(df, history_filename)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nCtrl+C detected. Exiting the program.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d7317-9ca3-49d2-98a7-2c34ed2b4aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}